import requests
import asyncio
from typing import List, Optional, AsyncGenerator, Dict
from crawl4ai import AsyncWebCrawler
from urllib.parse import urlencode, urlparse, parse_qs
import re


class SessionAwareJPXScraper:
    def __init__(self):
        self.base_url = "https://www2.jpx.co.jp/tseHpFront/JJK020020Action.do"
        self.form_url = "https://www2.jpx.co.jp/tseHpFront/JJK020010Action.do"
        self.session = requests.Session()
        self.jsessionid = None

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

        self.market_segments = {
            'prime': '011',
            'standard': '012',
            'growth': '013'
        }

    def initialize_session(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Å—Å–∏—é –∏ –ø–æ–ª—É—á–∞–µ—Ç JSESSIONID"""
        try:
            print("üîë –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏...")

            # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ä–º—É –ø–æ–∏—Å–∫–∞
            response = self.session.get(self.form_url)
            response.raise_for_status()

            print(f"‚úÖ –°—Ç–∞—Ç—É—Å: {response.status_code}")
            print(f"‚úÖ –†–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSESSIONID –∏–∑ cookies
            jsessionid_cookie = None
            for cookie in self.session.cookies:
                if cookie.name == 'JSESSIONID':
                    jsessionid_cookie = cookie.value
                    break

            if jsessionid_cookie:
                self.jsessionid = jsessionid_cookie
                print(f"‚úÖ JSESSIONID –ø–æ–ª—É—á–µ–Ω: {self.jsessionid[:20]}...")
            else:
                print("‚ö†Ô∏è JSESSIONID –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ cookies")

            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º URL redirect —Å jsessionid
            if ';jsessionid=' in response.url:
                url_jsessionid = response.url.split(';jsessionid=')[1].split('?')[0]
                if url_jsessionid:
                    self.jsessionid = url_jsessionid
                    print(f"‚úÖ JSESSIONID –∏–∑ URL: {self.jsessionid[:20]}...")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            with open('session_form.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("üìÑ –§–æ—Ä–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ session_form.html")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏: {e}")
            return False

    def build_session_form_data(self,
                                company_count: int = 50,
                                market_segments: List[str] = None) -> List[tuple]:
        """–°—Ç—Ä–æ–∏—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã"""

        if market_segments is None:
            market_segments = ['prime', 'standard']

        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        form_data = [
            ('ListShow', 'ListShow'),
            ('sniMtGmnId', ''),
            ('dspSsuPd', str(company_count)),
            ('dspSsuPdMapOut', '10>10<50>50<100>100<200>200<'),
            ('mgrMiTxtBx', ''),
            ('eqMgrCd', ''),
            ('hnsShzitPd', '+'),  # –í—Å–µ –ª–æ–∫–∞—Ü–∏–∏
            ('hnsShzitPdMapOut',
             '+><01>Hokkaido<02>Aomori<03>Iwate<04>Miyagi<05>Akita<06>Yamagata<07>Fukushima<08>Ibaraki<09>Tochigi<10>Gunma<11>Saitama<12>Chiba<13>Tokyo<14>Kanagawa<15>Niigata<16>Toyama<17>Ishikawa<18>Fukui<19>Yamanashi<20>Nagano<21>Gifu<22>Shizuoka<23>Aichi<24>Mie<25>Shiga<26>Kyoto<27>Osaka<28>Hyogo<29>Nara<30>Wakayama<31>Tottori<32>Shimane<33>Okayama<34>Hiroshima<35>Yamaguchi<36>Tokushima<37>Kagawa<38>Ehime<39>Kochi<40>Fukuoka<41>Saga<42>Nagasaki<43>Kumamoto<44>Oita<45>Miyazaki<46>Kagoshima<47>Okinawa<')
        ]

        # Market segments
        for segment in market_segments:
            if segment in self.market_segments:
                form_data.append(('szkbuChkbx', self.market_segments[segment]))

        form_data.append(('szkbuChkbxMapOut',
                          '011>Prime<012>Standard<013>Growth<008>TOKYO PRO Market<bj1>Ôºç<be1>Ôºç<111>Prime Foreign Stocks<112>Standard Foreign Stocks<113>Growth Foreign Stocks<bj2>Ôºç<be2>Ôºç<ETF>ETFs<ETN>ETNs<RET>Real Estate Investment Trusts (REITs)<IFD>Infrastructure Funds<999>Others<'))

        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        form_data.extend([
            ('gyshKbnPd', '+'),  # –í—Å–µ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏
            ('gyshKbnPdMapOut',
             '+><0050>Fishery, Agriculture &amp; Forestry<1050>Mining<2050>Construction<3050>Foods<3100>Textiles &amp; Apparels<3150>Pulp &amp; Paper<3200>Chemicals<3250>Pharmaceutical<3300>Oil &amp; Coal Products<3350>Rubber Products<3400>Glass &amp; Ceramics Products<3450>Iron &amp; Steel<3500>Nonferrous Metals<3550>Metal Products<3600>Machinery<3650>Electric Appliances<3700>Transportation Equipment<3750>Precision Instruments<3800>Other Products<4050>Electric Power &amp; Gas<5050>Land Transportation<5100>Marine Transportation<5150>Air Transportation<5200>Warehousing &amp; Harbor Transportation Services<5250>Information &amp; Communication<6050>Wholesale Trade<6100>Retail Trade<7050>Banks<7100>Securities &amp; Commodity Futures<7150>Insurance<7200>Other Financing Business<8050>Real Estate<9050>Services<9999>Nonclassifiable<'),
        ])

        return form_data

    def scrape_with_session(self, **kwargs) -> str:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–µ–π"""

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é
        if not self.initialize_session():
            return ""

        form_data = self.build_session_form_data(**kwargs)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º URL —Å jsessionid –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        target_url = self.base_url
        if self.jsessionid:
            target_url = f"{self.base_url};jsessionid={self.jsessionid}"

        headers = {
            'Content-Type': 'application/x-www-form-urlencoded',
            'Origin': 'https://www2.jpx.co.jp',
            'Referer': f"{self.form_url};jsessionid={self.jsessionid}" if self.jsessionid else self.form_url,
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1'
        }

        post_data_string = urlencode(form_data)
        print(f"\nüöÄ [SESSION SEARCH] POST –¥–∞–Ω–Ω—ã–µ ({len(post_data_string)} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"üöÄ Target URL: {target_url}")
        print(f"üöÄ JSESSIONID: {self.jsessionid[:20] if self.jsessionid else '–ù–ï–¢'}...")
        print(f"üöÄ Referer: {headers['Referer']}")

        szkbu_count = post_data_string.count('szkbuChkbx=')
        print(f"üöÄ Market segments: {szkbu_count}")

        try:
            response = self.session.post(
                target_url,
                data=form_data,
                headers=headers
            )
            response.raise_for_status()

            print(f"‚úÖ –°—Ç–∞—Ç—É—Å: {response.status_code}")
            print(f"‚úÖ –†–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"‚úÖ Final URL: {response.url}")

            if "‰ª∂‰∏≠" in response.text:
                print("üéâ –ù–ê–ô–î–ï–ù–´ –î–ê–ù–ù–´–ï –û –ö–û–ú–ü–ê–ù–ò–Ø–•!")

                match = re.search(r'Display of (\d+)-(\d+) items/(\d+)', response.text)
                if match:
                    start, end, total = match.groups()
                    print(f"üìä –ü–æ–∫–∞–∑–∞–Ω–æ: {start}-{end} –∏–∑ {total} –∫–æ–º–ø–∞–Ω–∏–π")
                else:
                    match = re.search(r'(\d+)‰ª∂‰∏≠', response.text)
                    if match:
                        print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {match.group(1)} –∫–æ–º–ø–∞–Ω–∏–π")

                with open('session_search_success.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ session_search_success.html")

                return response.text
            else:
                print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                with open('session_search_error.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print("üíæ –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ session_search_error.html")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")

        return ""


# Crawl4AI –≤–µ—Ä—Å–∏—è —Å —Å–µ—Å—Å–∏–µ–π
async def crawl_jpx_with_session(
        company_count: int = 50,
        market_segments: List[str] = None,
        max_pages: Optional[int] = None,
        delay: float = 1.0
) -> AsyncGenerator[str, None]:
    """Crawl4AI –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å–µ—Å—Å–∏–∏"""

    if market_segments is None:
        market_segments = ['prime', 'standard']

    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —Å–µ—Å—Å–∏—é —á–µ—Ä–µ–∑ requests
    scraper = SessionAwareJPXScraper()
    if not scraper.initialize_session():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Å—Å–∏—é")
        return

    form_data = scraper.build_session_form_data(
        company_count=company_count,
        market_segments=market_segments
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        page = 0

        while True:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
            current_form_data = form_data.copy()
            if page > 0:
                current_form_data.append(('pageOffset', str(page * company_count)))

            post_data_string = urlencode(current_form_data)

            # URL —Å jsessionid
            target_url = scraper.base_url
            if scraper.jsessionid:
                target_url = f"{scraper.base_url};jsessionid={scraper.jsessionid}"

            print(f"\nü§ñ [CRAWL4AI SESSION] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}")
            print(f"ü§ñ Target URL: {target_url}")
            print(f"ü§ñ JSESSIONID: {scraper.jsessionid[:20] if scraper.jsessionid else '–ù–ï–¢'}...")

            try:
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if page == 0:
                    print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è crawl4ai —Å–µ—Å—Å–∏–∏...")
                    init_url = scraper.form_url
                    if scraper.jsessionid:
                        init_url = f"{scraper.form_url};jsessionid={scraper.jsessionid}"

                    init_result = await crawler.arun(
                        url=init_url,
                        method="GET"
                    )

                    if not init_result.success:
                        print("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ crawl4ai")
                        break

                    print("‚úÖ Crawl4ai —Å–µ—Å—Å–∏—è –≥–æ—Ç–æ–≤–∞")

                # POST –∑–∞–ø—Ä–æ—Å
                result = await crawler.arun(
                    url=target_url,
                    method="POST",
                    data=post_data_string,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Origin': 'https://www2.jpx.co.jp',
                        'Referer': f"{scraper.form_url};jsessionid={scraper.jsessionid}" if scraper.jsessionid else scraper.form_url,
                        'Cache-Control': 'max-age=0',
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Sec-Ch-Ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
                        'Sec-Ch-Ua-Mobile': '?0',
                        'Sec-Ch-Ua-Platform': '"macOS"',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'same-origin',
                        'Sec-Fetch-User': '?1',
                        'Upgrade-Insecure-Requests': '1'
                    }
                )

                if not result.success:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}")
                    break

                html_content = result.html

                if "‰ª∂‰∏≠" not in html_content:
                    print(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}")
                    with open(f'crawl4ai_session_debug_{page + 1}.html', 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    break

                print(f"‚úÖ –£—Å–ø–µ—Ö! –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}, —Ä–∞–∑–º–µ—Ä: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if "‰ª∂‰∏≠" in html_content:
                    match = re.search(r'Display of (\d+)-(\d+) items/(\d+)', html_content)
                    if match:
                        start, end, total = match.groups()
                        print(f"üìä –ü–æ–∫–∞–∑–∞–Ω–æ: {start}-{end} –∏–∑ {total} –∫–æ–º–ø–∞–Ω–∏–π")

                with open(f'crawl4ai_session_success_{page + 1}.html', 'w', encoding='utf-8') as f:
                    f.write(html_content)

                yield html_content

                if max_pages and page + 1 >= max_pages:
                    break

                page += 1
                await asyncio.sleep(delay)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}: {e}")
                break


# –¢–µ—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–µ–π
async def test_session_methods():
    print("üîë –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –ü–†–ê–í–ò–õ–¨–ù–û–ô –°–ï–°–°–ò–ï–ô")
    print("=" * 70)

    # –¢–µ—Å—Ç 1: Requests —Å —Å–µ—Å—Å–∏–µ–π
    print("\n1Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º Requests —Å —Å–µ—Å—Å–∏–µ–π...")
    scraper = SessionAwareJPXScraper()
    html_content = scraper.scrape_with_session(
        company_count=25,
        market_segments=['prime', 'standard']
    )

    requests_success = "‰ª∂‰∏≠" in html_content if html_content else False

    # –¢–µ—Å—Ç 2: Crawl4AI —Å —Å–µ—Å—Å–∏–µ–π
    print("\n2Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º Crawl4AI —Å —Å–µ—Å—Å–∏–µ–π...")
    crawl4ai_success = False

    async for html_page in crawl_jpx_with_session(
            company_count=25,
            market_segments=['prime', 'standard'],
            max_pages=1
    ):
        if "‰ª∂‰∏≠" in html_page:
            crawl4ai_success = True
        break

    print("\n" + "=" * 70)
    print("üèÅ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –°–ï–°–°–ò–ï–ô")
    print("=" * 70)
    print(f"üìä Session Requests:  {'‚úÖ –†–ê–ë–û–¢–ê–ï–¢' if requests_success else '‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢'}")
    print(f"ü§ñ Session Crawl4AI:  {'‚úÖ –†–ê–ë–û–¢–ê–ï–¢' if crawl4ai_success else '‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢'}")

    if requests_success and crawl4ai_success:
        print("\nüéâ –û–ë–ê –ú–ï–¢–û–î–ê –° –°–ï–°–°–ò–ï–ô –†–ê–ë–û–¢–ê–Æ–¢!")
        print("üîë –ü—Ä–æ–±–ª–µ–º–∞ –±—ã–ª–∞ –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ JSESSIONID")
    elif requests_success:
        print("\n‚ö†Ô∏è –¢–æ–ª—å–∫–æ Requests —Å —Å–µ—Å—Å–∏–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print("üîß Crawl4AI –Ω—É–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–ª–∞–¥–∏—Ç—å")
    else:
        print("\n‚ùå –û–±–∞ –º–µ—Ç–æ–¥–∞ –≤—Å–µ –µ—â–µ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç")
        print("üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã session_* –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")


if __name__ == "__main__":
    asyncio.run(test_session_methods())