import asyncio
from typing import List, Optional, AsyncGenerator
from crawl4ai import AsyncWebCrawler
from urllib.parse import urlencode
from jpx_scraper import SessionAwareJPXScraper


async def crawl_jpx_complete_detailed(
        company_count: int = 100,
        market_segments: List[str] = None,
        industries: List[str] = None,
        locations: List[str] = None,
        max_pages: Optional[int] = None,
        delay: float = 1.0
) -> AsyncGenerator[str, None]:
    """–ü–æ–ª–Ω–∞—è crawl4ai –≤–µ—Ä—Å–∏—è detailed search"""

    if market_segments is None:
        market_segments = ['prime', 'standard']

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º requests scraper –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è form data
    scraper = SessionAwareJPXScraper()
    form_data = scraper.build_detailed_form_data(
        company_count=company_count,
        market_segments=market_segments,
        industries=industries,
        locations=locations
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        page = 0

        while True:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
            current_form_data = form_data.copy()
            if page > 0:
                current_form_data.append(('pageOffset', str(page * company_count)))

            post_data_string = urlencode(current_form_data)
            print(f"\nü§ñ [CRAWL4AI DETAILED] –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}")
            print(f"ü§ñ POST –¥–∞–Ω–Ω—ã–µ ({len(post_data_string)} —Å–∏–º–≤–æ–ª–æ–≤)")

            try:
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if page == 0:
                    print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏...")
                    init_result = await crawler.arun(
                        url="https://www2.jpx.co.jp/tseHpFront/JJK020010Action.do",
                        method="GET"
                    )

                    if not init_result.success:
                        print("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏")
                        break

                    print("‚úÖ –°–µ—Å—Å–∏—è –≥–æ—Ç–æ–≤–∞")

                # POST –∑–∞–ø—Ä–æ—Å
                result = await crawler.arun(
                    url="https://www2.jpx.co.jp/tseHpFront/JJK020020Action.do",
                    method="POST",
                    data=post_data_string,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Origin': 'https://www2.jpx.co.jp',
                        'Referer': 'https://www2.jpx.co.jp/tseHpFront/JJK020010Action.do',
                        'Cache-Control': 'max-age=0',
                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Sec-Ch-Ua': '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
                        'Sec-Ch-Ua-Mobile': '?0',
                        'Sec-Ch-Ua-Platform': '"macOS"',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'same-origin',
                        'Sec-Fetch-User': '?1',
                        'Upgrade-Insecure-Requests': '1'
                    }
                )

                if not result.success:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}")
                    break

                html_content = result.html

                if "‰ª∂‰∏≠" not in html_content:
                    print(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}")
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    with open(f'crawl4ai_debug_detailed_{page + 1}.html', 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    break

                print(f"‚úÖ –£—Å–ø–µ—Ö! –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}, —Ä–∞–∑–º–µ—Ä: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if "‰ª∂‰∏≠" in html_content:
                    match = re.search(r'Display of (\d+)-(\d+) items/(\d+)', html_content)
                    if match:
                        start, end, total = match.groups()
                        print(f"üìä –ü–æ–∫–∞–∑–∞–Ω–æ: {start}-{end} –∏–∑ {total} –∫–æ–º–ø–∞–Ω–∏–π")
                    else:
                        match = re.search(r'(\d+)‰ª∂‰∏≠', html_content)
                        if match:
                            print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {match.group(1)} –∫–æ–º–ø–∞–Ω–∏–π")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                with open(f'crawl4ai_detailed_success_{page + 1}.html', 'w', encoding='utf-8') as f:
                    f.write(html_content)
                print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ crawl4ai_detailed_success_{page + 1}.html")

                yield html_content

                if max_pages and page + 1 >= max_pages:
                    print(f"‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç: {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
                    break

                page += 1
                await asyncio.sleep(delay)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page + 1}: {e}")
                break


# –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤
async def test_complete_methods():
    print("üöÄ –ü–û–õ–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï DETAILED SEARCH")
    print("=" * 80)

    # –¢–µ—Å—Ç 1: Requests
    print("\n1Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º Complete Requests Detailed Search...")
    scraper = SessionAwareJPXScraper()
    html_content = scraper.scrape_detailed(
        company_count=25,
        market_segments=['prime'],
        industries=['information_communication']
    )

    requests_success = "‰ª∂‰∏≠" in html_content if html_content else False

    # –¢–µ—Å—Ç 2: Crawl4AI
    print("\n2Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º Complete Crawl4AI Detailed Search...")
    crawl4ai_success = False
    page_count = 0

    async for html_page in crawl_jpx_complete_detailed(
            company_count=25,
            market_segments=['prime'],
            industries=['information_communication'],
            max_pages=1
    ):
        page_count += 1
        if "‰ª∂‰∏≠" in html_page:
            crawl4ai_success = True
        break

    print("\n" + "=" * 80)
    print("üèÅ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–õ–ù–û–ì–û DETAILED SEARCH")
    print("=" * 80)
    print(f"üìä Complete Requests:  {'‚úÖ –†–ê–ë–û–¢–ê–ï–¢' if requests_success else '‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢'}")
    print(f"ü§ñ Complete Crawl4AI:  {'‚úÖ –†–ê–ë–û–¢–ê–ï–¢' if crawl4ai_success else '‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢'}")

    if requests_success and crawl4ai_success:
        print("\nüéâ –û–ë–ê –ú–ï–¢–û–î–ê –ü–û–õ–ù–û–ì–û DETAILED SEARCH –†–ê–ë–û–¢–ê–Æ–¢!")
        print("üéØ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö JPX")
        print("üí° –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ:")
        print("   - Market segments (Prime, Standard, Growth, etc.)")
        print("   - Industries (IT/Communication, Banking, etc.)")
        print("   - Locations (Tokyo, Osaka, etc.)")
        print("   - Trading units, Fiscal year-end, –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ")
    elif requests_success:
        print("\n‚ö†Ô∏è –¢–æ–ª—å–∫–æ Complete Requests —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print("üîß –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ requests –≤–µ—Ä—Å–∏—é –∏–ª–∏ –æ—Ç–ª–∞–¥—å—Ç–µ crawl4ai")
    else:
        print("\n‚ùå –ù–∏ –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print("üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã POST –∑–∞–ø—Ä–æ—Å–∞")
        print("üí° –°—Ä–∞–≤–Ω–∏—Ç–µ —Å —É—Å–ø–µ—à–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º –≤ Insomnia")


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤
async def demo_filters():
    print("üé® –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ó–õ–ò–ß–ù–´–• –§–ò–õ–¨–¢–†–û–í")
    print("=" * 60)

    scraper = SessionAwareJPXScraper()

    # –î–µ–º–æ 1: IT –∫–æ–º–ø–∞–Ω–∏–∏ –≤ Tokyo
    print("\nüè¢ IT –∫–æ–º–ø–∞–Ω–∏–∏ –≤ Tokyo...")
    async for page in crawl_jpx_complete_detailed(
            company_count=20,
            market_segments=['prime'],
            industries=['information_communication'],
            locations=['tokyo'],
            max_pages=1
    ):
        print("‚úÖ –ü–æ–ª—É—á–µ–Ω—ã IT –∫–æ–º–ø–∞–Ω–∏–∏ Tokyo")
        break

    # –î–µ–º–æ 2: –ë–∞–Ω–∫–∏ –≤–æ –≤—Å–µ—Ö –ª–æ–∫–∞—Ü–∏—è—Ö
    print("\nüè¶ –ë–∞–Ω–∫–∏ –≤–æ –≤—Å–µ—Ö –ª–æ–∫–∞—Ü–∏—è—Ö...")
    async for page in crawl_jpx_complete_detailed(
            company_count=20,
            market_segments=['prime', 'standard'],
            industries=['banks'],
            max_pages=1
    ):
        print("‚úÖ –ü–æ–ª—É—á–µ–Ω—ã –±–∞–Ω–∫–∏")
        break

    # –î–µ–º–æ 3: –í—Å–µ Prime –∫–æ–º–ø–∞–Ω–∏–∏
    print("\n‚≠ê –í—Å–µ Prime –∫–æ–º–ø–∞–Ω–∏–∏...")
    async for page in crawl_jpx_complete_detailed(
            company_count=50,
            market_segments=['prime'],
            max_pages=1
    ):
        print("‚úÖ –ü–æ–ª—É—á–µ–Ω—ã Prime –∫–æ–º–ø–∞–Ω–∏–∏")
        break

    print("\nüéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Å—Ç
    asyncio.run(test_complete_methods())

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    # asyncio.run(demo_filters())